{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 29,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "\ntrain=pd.read_csv('../input/train_onehot.csv')\ntest=pd.read_csv('../input/test.csv')\n\ntrain_dir='../input/train_scaled/train_scaled/'\ntest_dir='../input/test_scaled/test_scaled/'\n\ntrain_images=np.array(train.iloc[:,0])\nimagearr=[]\nfor i in train_images:\n    img=Image.open(train_dir+i)\n    imagearr.append(np.array(img))\nX=np.array(imagearr)\n",
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ce9cdf6f4c34233f5b6303ab6d2eb2b816ce6da1",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_images=np.array(test.iloc[:,0])\nimagearr=[]\nfor i in test_images:\n    img=Image.open(test_dir+i)\n    imagearr.append(np.array(img))\nY=np.array(imagearr)",
      "execution_count": 31,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8d2c08f7043aad04d722d947c178044338a3502f",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "Y.shape",
      "execution_count": 32,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a2b815418cfcc6dda798994895b6ce0c151b9c5d",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\nplt.imshow(X[0].reshape(64,64))",
      "execution_count": 33,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "81d70883a388d621db31cf8ecf4252aee40db8c2",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(X[260].shape)\nprint(X.shape)\nprint(X.reshape(X.shape[0], 64, 64, 1).shape)\nX=X.reshape(-1, 64, 64, 1)",
      "execution_count": 34,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "83c4a6a4e396fa760a9909a7d414794e53fece35",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(X[0])\nX=X/X.max()",
      "execution_count": 35,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "41b9f2f521c9e1ee64c66e4aea96cd2f94de10d6",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "Y=Y.reshape(-1, 64, 64, 1)",
      "execution_count": 36,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d0450c0937152369b9a44836d09c9287484abaed",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "Y[0:20].shape",
      "execution_count": 37,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3f4015113a8f7ecfdf8c4bd7aa4fe733ec75c6d7",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\ntrain.iloc[0,1:].values",
      "execution_count": 38,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2822d98dd2c79025395b557f6e13390835a0540b",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_labels=[]\nfor i in range(len(train)):\n    X_labels.append(train.iloc[i,1:].values)\n",
      "execution_count": 39,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bb665f3a4079057f7cf561d11e4dc88ce124ced4",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "X_labels=np.array(X_labels)\nX_labels",
      "execution_count": 40,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e22bf3b3117b8c96bd7bb764ee3ad6678d687b3d",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "plt.imshow(X[5].reshape(64,64))\nprint(X_labels.shape)",
      "execution_count": 41,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d39c39ccd38bd432becef9c1a3a8417ee66493c4",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\ntest_size = 0.3\nX_train, X_test, Y_train, Y_test = train_test_split(X,X_labels, test_size=test_size, random_state=101)\n\nimg_size = 64\nchannel_size = 1\nprint(\"Training Size:\", X_train.shape)\nprint(X_train.shape[0],\"samples - \", X_train.shape[1],\"x\",X_train.shape[2],\"grayscale image\")\n\nprint(\"\\n\")\n\nprint(\"Test Size:\",X_test.shape)\nprint(X_test.shape[0],\"samples - \", X_test.shape[1],\"x\",X_test.shape[2],\"grayscale image\")",
      "execution_count": 42,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "00bf1f3334b164db06753f6dc883f568a2dd60af",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "class SignClass():\n    \n    def __init__(self):\n        self.i = 0\n        \n        self.training_images = X_train\n        self.training_labels = Y_train\n        \n        self.test_images = X_test\n        self.test_labels = Y_test\n        self._epochs_completed = 0\n        self._index_in_epoch = 0\n        \n        self._num_examples = X_train.shape[0]\n    \n\n\n        \n    def next_batch(self, batch_size,fake_data=False, shuffle=True):\n#         x = self.training_images[self.i:self.i+batch_size].reshape(-1,64,64,1)\n#         y = self.training_labels[self.i:self.i+batch_size]\n#         self.i = (self.i + batch_size) % len(self.training_images)\n#         return x, y\n\n        if fake_data:\n            fake_image = [1] * 4096\n            if self.one_hot:\n                fake_label = [1] + [0] * 9\n            else:\n                fake_label = 0\n            return [fake_image for _ in xrange(batch_size)], [fake_label for _ in xrange(batch_size)]\n        start = self._index_in_epoch\n        # Shuffle for the first epoch\n        if self._epochs_completed == 0 and start == 0 and shuffle:\n            perm0 = np.arange(self._num_examples)\n            np.random.shuffle(perm0)\n            self.training_images = self.training_images[perm0]\n            self.training_labels = self.training_labels[perm0]\n        # Go to the next epoch\n        if start + batch_size > self._num_examples:\n        # Finished epoch\n            self._epochs_completed += 1\n            # Get the rest examples in this epoch\n            rest_num_examples = self._num_examples - start\n            images_rest_part = self.training_images[start:self._num_examples]\n            labels_rest_part = self.training_labels[start:self._num_examples]\n          # Shuffle the data\n            if shuffle:\n                perm = np.arange(self._num_examples)\n                np.random.shuffle(perm)\n                self.training_images = self.training_images[perm]\n                self.training_labels = self.training_labels[perm]\n            # Start next epoch\n            start = 0\n            self._index_in_epoch = batch_size - rest_num_examples\n            end = self._index_in_epoch\n            images_new_part = self.training_images[start:end]\n            labels_new_part = self.training_labels[start:end]\n            return np.concatenate((images_rest_part, images_new_part), axis=0), np.concatenate((labels_rest_part, labels_new_part), axis=0)\n        else:\n            self._index_in_epoch += batch_size\n            end = self._index_in_epoch\n            return self.training_images[start:end], self.training_labels[start:end]",
      "execution_count": 43,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9f3b329cc7dff8e41b8af9f7b97f1be116b06b8c",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "ch = SignClass()",
      "execution_count": 44,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5c27165a88fb1bc720601212da5359c42afe842a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import tensorflow as tf",
      "execution_count": 45,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "026871a3633d3c75c713e13f22c5a44153d57765",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "x = tf.placeholder(tf.float32,shape=[None,64,64,1],name=\"x\")\ny_true = tf.placeholder(tf.float32,shape=[None,30],name=\"y_true\")",
      "execution_count": 46,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1af27e2724d162c06b160dab664cd84aec19d4c0",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "hold_prob = tf.placeholder(tf.float32,name=\"hold_prob\")\ntf.summary.scalar('dropout_keep_probability', hold_prob)",
      "execution_count": 47,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "efee5d9e2c5585063c6f47205b9075f697b374c8",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "def init_weights(shape):\n    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(init_random_dist,name=\"W\")\n\ndef init_bias(shape):\n    init_bias_vals = tf.constant(0.1, shape=shape)\n    return tf.Variable(init_bias_vals,name=\"b\")\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\ndef max_pool_2by2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                          strides=[1, 2, 2, 1], padding='SAME')\n\ndef convolutional_layer(input_x, shape,name=\"conv_layer\"):\n    with tf.name_scope(name):\n        W = init_weights(shape)\n        b = init_bias([shape[3]])\n        c=conv2d(input_x, W)\n        act=tf.nn.relu(c + b)\n        tf.summary.histogram(\"weights\",W)\n        tf.summary.histogram(\"biases\",b)\n        tf.summary.histogram(\"activations\",act)\n        return act\n\ndef normal_full_layer(input_layer, size,name=\"normal_layer\"):\n    with tf.name_scope(name):\n        input_size = int(input_layer.get_shape()[1])\n        W = init_weights([input_size, size])\n        b = init_bias([size])\n        return tf.matmul(input_layer, W) + b",
      "execution_count": 48,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0d8dbff850ef5d5afcc9fbfde2e3ab754dc84801",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "convo_1 = convolutional_layer(x,shape=[4,4,1,64])\nconvo_1_pooling = max_pool_2by2(convo_1)",
      "execution_count": 49,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5e277cf91e41449f49c0d443dccf9f42af6edc63",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "convo_2 = convolutional_layer(convo_1_pooling,shape=[4,4,64,128])\nconvo_2_pooling = max_pool_2by2(convo_2)",
      "execution_count": 50,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "91496d6bb65f8478483e59ca7b5ab3523a9afe79",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "convo_2_flat = tf.reshape(convo_2_pooling,[-1,16*16*128])\nfull_layer_one = tf.nn.relu(normal_full_layer(convo_2_flat,1024))",
      "execution_count": 51,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "58707eee96fd38df3e113d9b4e5d726fb70d0dd5",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "full_one_dropout = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)",
      "execution_count": 52,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "112ed9501157eccf0b7968647f3054b671fc0367",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_pred = normal_full_layer(full_one_dropout,30)",
      "execution_count": 53,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "93167722912289445915959188529adcc3d8ca02",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "with tf.name_scope(\"crossentropy\"):\n    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true,logits=y_pred))\n    tf.summary.scalar('cross_entropy', cross_entropy)\nwith tf.name_scope(\"optimizer\"):\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n    train = optimizer.minimize(cross_entropy)\ninit = tf.global_variables_initializer()\n",
      "execution_count": 54,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e2d87b3677d8dc01896dab1d28c40ff560141a0f",
        "scrolled": true,
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "steps = 500\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    \n    sess.run(init)\n    merged_Summary=tf.summary.merge_all()\n    writer=tf.summary.FileWriter('dir3/')\n    writer.add_graph(sess.graph)\n    for i in range(steps):\n        batch = ch.next_batch(1000)\n        \n        sess.run(train, feed_dict={x: batch[0], y_true: batch[1], hold_prob: 0.5})\n        \n        # PRINT OUT A MESSAGE EVERY 100 STEPS\n        if i%100 == 0:\n            s=sess.run(merged_Summary,feed_dict={x: batch[0], y_true: batch[1], hold_prob: 0.5})\n            writer.add_summary(s,i)\n            print('Currently on step {}'.format(i))\n            print('Accuracy is:')\n            # Test the Train Model\n            with tf.name_scope(\"accuracy\"):\n                matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_true,1))\n\n                acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n                tf.summary.scalar('accuracy', acc)\n                print(sess.run(acc,feed_dict={x:ch.test_images,y_true:ch.test_labels,hold_prob:1.0}))\n                print('\\n')\n    save_path = saver.save(sess, \"/tmp/model.ckpt\")\n    print(\"Model saved in path: %s\" % save_path)\n    ",
      "execution_count": 55,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6e76cd62353912bad194df5bb3479d4fd8b4f965",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#import tensorflow as tf\npredictions=[]\nwith tf.Session() as sess:\n  # Restore variables from disk.\n    saver.restore(sess, \"/tmp/model.ckpt\")\n    print('predicting')\n    predicta=tf.argmax(y_pred,1)\n    predicts=tf.nn.softmax(y_pred)\n    print(1)\n#     print(sess.run(tf.Print(y_pred,[y_pred])))\n#     print(sess.run(tf.Print(predicta,[predicta])))\n#     print(sess.run(tf.Print(predicts,[predicts])))\n    \n    predictions1=sess.run(y_pred,feed_dict={x:Y[0:1000],hold_prob:1.0})\n    predictions2=sess.run(y_pred,feed_dict={x:Y[1000:2000],hold_prob:1.0})\n    predictions3=sess.run(y_pred,feed_dict={x:Y[2000:3000],hold_prob:1.0})\n    predictions4=sess.run(y_pred,feed_dict={x:Y[3000:4000],hold_prob:1.0})\n    predictions5=sess.run(y_pred,feed_dict={x:Y[4000:5000],hold_prob:1.0})\n    predictions6=sess.run(y_pred,feed_dict={x:Y[5000:],hold_prob:1.0})\n\n# for j in [predictions1,predictions2,predictions3,predictions4,predictions5,predictions6]:\n    \n#     predictions.append(j)\n#predictions=predictions1+predictions2+predictions3+predictions4+predictions5+predictions6\n# for j in predictions1:\n#     predictions.append([format(float(x), '.16f') for x in j])\n# for j in predictions2:\n#     predictions.append([format(float(x), '.16f') for x in j])\n# for j in predictions3:\n#     predictions.append([format(float(x), '.16f') for x in j])\n# for j in predictions4:\n#     predictions.append([format(float(x), '.16f') for x in j])\n# for j in predictions5:\n#     predictions.append([format(float(x), '.16f') for x in j])\n# for j in predictions6:\n#     predictions.append([format(float(x), '.16f') for x in j])\n\nfor j in predictions1:\n    predictions.append([format(float(x), '.16f') for x in j])\nfor j in predictions2:\n    predictions.append([format(float(x), '.16f') for x in j])\nfor j in predictions3:\n    predictions.append([format(float(x), '.16f') for x in j])\nfor j in predictions4:\n    predictions.append([format(float(x), '.16f') for x in j])\nfor j in predictions5:\n    predictions.append([format(float(x), '.16f') for x in j])\nfor j in predictions6:\n    predictions.append([format(float(x), '.16f') for x in j])\npredictions=np.array(predictions)\nprint('done >',predictions.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d87f3527879eac412101057e4b6a6d42fbee51c6",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "predictions.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "89bb9c071fe8f1c2f9ade807976c87c9189d3e21",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "trainmain=pd.read_csv('../input/train_onehot.csv')\ntrainmain.drop('Image_id',1).columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6e3d73ad7eafd8a4e55edaac5f03c33103af36bd",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "pr=pd.DataFrame(data=predictions,columns=trainmain.drop('Image_id',1).columns)\nmmm=pd.concat([pd.DataFrame(data=test_images,columns=['image_id']),pr],axis=1)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f617cfb3f9740f67224a4900e32129429d5576a7",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "mmm.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5a16fa0db22808c2492512fefc28198433fc1698",
        "collapsed": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "mmm.to_csv('pr11.csv',index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7c68b4f5efa239b8d471b5a5183c7408dfd02c26",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "mmm.iloc[1,:]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "94f7ce3cca5c7613bdd14e8c3f2568743ff8429e",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "mmm.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "43b02b7d4d51f88b3bd24b58c89398d974d44758",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "predictions1.astype(float)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "26424b169abe12969be25ecc69af49034736f54c"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}